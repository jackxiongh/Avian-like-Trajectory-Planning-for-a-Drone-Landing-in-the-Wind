{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This script loads quadrotor flight data in different wind conditions, trains a wind invariant representation of the unmodeled aerodynamics, and tests the performance of the model when adapting to new data in different wind conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:03.269294Z",
     "start_time": "2024-11-05T17:12:01.414607Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import utils\n",
    "import mlmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:04.811401Z",
     "start_time": "2024-11-05T17:12:04.808535Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    NUM_WORKERS = 0 # Windows does not support multiprocessing\n",
    "else:\n",
    "    NUM_WORKERS = 2\n",
    "print('running on ' + sys.platform + ', setting ' + str(NUM_WORKERS) + ' workers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create some simple visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:07.089682Z",
     "start_time": "2024-11-05T17:12:07.086497Z"
    }
   },
   "outputs": [],
   "source": [
    "dim_a = 3\n",
    "features = ['v', 'q', 'pwm']\n",
    "label = 'wind_effect'\n",
    "\n",
    "# Training data collected from the neural-fly drone\n",
    "dataset = 'neural-fly' \n",
    "dataset_folder = ''\n",
    "hover_pwm_ratio = 1.\n",
    "\n",
    "# # Training data collected from an intel aero drone\n",
    "# dataset = 'neural-fly-transfer'\n",
    "# dataset_folder = 'data/training-transfer'\n",
    "# hover_pwm = 910 # mean hover pwm for neural-fly drone\n",
    "# intel_hover_pwm = 1675 # mean hover pwm for intel-aero drone\n",
    "# hover_pwm_ratio = hover_pwm / intel_hover_pwm # scaling ratio from system id\n",
    "\n",
    "modelname = f\"{dataset}_dim-a-{dim_a}_{'-'.join(features)}\" # 'intel-aero_fa-num-Tsp_v-q-pwm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:11.657601Z",
     "start_time": "2024-11-05T17:12:10.537747Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RawData = utils.load_data(dataset_folder)\n",
    "Data = utils.format_data(RawData, features=features, output=label)\n",
    "\n",
    "testdata_folder = 'data/resources/csv_res_0116_4/test'\n",
    "# RawData = utils.load_data(testdata_folder, expnames='(baseline_)([0-9]*|no)wind')\n",
    "RawData = utils.load_data(testdata_folder, expnames='')\n",
    "TestData = utils.format_data(RawData, features=features, output=label, hover_pwm_ratio=hover_pwm_ratio) # wind condition label, C, will not make sense for this data - that's okay since C is only used in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T06:32:20.217415Z",
     "start_time": "2024-11-04T06:32:17.123054Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in Data:\n",
    "    utils.plot_subdataset(data, features, title_prefix=\"(Training data)\")\n",
    "\n",
    "for data in TestData:\n",
    "    utils.plot_subdataset(data, features, title_prefix=\"(Testing Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:16.274146Z",
     "start_time": "2024-11-05T17:12:16.270921Z"
    }
   },
   "outputs": [],
   "source": [
    "options = {}\n",
    "options['dim_x'] = Data[0].X.shape[1]\n",
    "options['dim_y'] = Data[0].Y.shape[1]\n",
    "options['num_c'] = len(Data)\n",
    "print('dims of (x, y) are', (options['dim_x'], options['dim_y']))\n",
    "print('there are ' + str(options['num_c']) + ' different conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:12:17.311103Z",
     "start_time": "2024-11-05T17:12:17.307745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "options['features'] = features\n",
    "options['dim_a'] = dim_a\n",
    "options['loss_type'] = 'crossentropy-loss'\n",
    "\n",
    "options['shuffle'] = True # True: shuffle trajectories to data points\n",
    "options['K_shot'] = 32 # number of K-shot for least square on a\n",
    "options['phi_shot'] = 256 # batch size for training phi\n",
    "\n",
    "options['alpha'] = 0.01 # adversarial regularization loss\n",
    "options['learning_rate'] = 5e-4\n",
    "options['frequency_h'] = 2 # how many times phi is updated between h updates, on average\n",
    "options['SN'] = 2. # maximum single layer spectral norm of phi\n",
    "options['gamma'] = 10. # max 2-norm of a\n",
    "options['num_epochs'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaptation dataset will be used to update $a$ in each training loop.\n",
    "The training dataset will be used to train $\\phi$ in each training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T06:32:30.643699Z",
     "start_time": "2024-11-04T06:32:30.637239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trainset = []\n",
    "# Adaptset = []\n",
    "Trainloader = []\n",
    "Adaptloader = []\n",
    "for i in range(options['num_c']):\n",
    "    fullset = mlmodel.MyDataset(Data[i].X, Data[i].Y, Data[i].C)\n",
    "    \n",
    "    l = len(Data[i].X)\n",
    "    if options['shuffle']:\n",
    "        trainset, adaptset = random_split(fullset, [int(2/3*l), l-int(2/3*l)])\n",
    "    else:\n",
    "        trainset = mlmodel.MyDataset(Data[i].X[:int(2/3*l)], Data[i].Y[:int(2/3*l)], Data[i].C) \n",
    "        adaptset = mlmodel.MyDataset(Data[i].X[int(2/3*l):], Data[i].Y[int(2/3*l):], Data[i].C)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=options['phi_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "    adaptloader = torch.utils.data.DataLoader(adaptset, batch_size=options['K_shot'], shuffle=options['shuffle'], num_workers=NUM_WORKERS)\n",
    "   \n",
    "    # Trainset.append(trainset)\n",
    "    # Adaptset.append(adaptset)\n",
    "    Trainloader.append(trainloader) # for training phi\n",
    "    Adaptloader.append(adaptloader) # for LS on a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adversarially Invariant Meta Learning\n",
    "\n",
    "Assume the state $x\\in\\mathbb{R}^n$ and $c$ is hidden state used to represent changing environment. We are interested in learning some function $f(x(t),c(t))$. $f(x(t),c(t))$ can be separated into three terms: $$f(x(t),c(t))=\\phi(x(t))a(c(t))+d(t),$$\n",
    "where $\\phi(x(t))$ captures the $c$-variant part and $a(c(t))\\in\\mathbb{R}^m$ is implicitly a function of the hidden state $c(t)$. Finally, $d(t)$ is the residual noise term.\n",
    "\n",
    "We want to learn $\\phi(x)$ such that it doesn't include any information about $c$. To reach this goal, we introduce another neural network $h$ where $h(\\phi(x))$ tries to predict $c$.\n",
    "\n",
    "The loss function is given as\n",
    "$$\\max_h\\min_{\\phi, \\left\\{a_{c_j}\\right\\}_j}\\sum_{j}\\sum_{i}\\left\\|\\phi(x^{(i)}_{c_j})a_{c_j}-f(x^{(i)}_{c_j},c_j)\\right\\|^2-\\alpha\\cdot\\text{CrossEntropy}\\left(h(\\phi(x^{(i)}_{c_j})),j\\right)$$\n",
    "Note that the $\\text{CrossEntropy-loss}$ will not require physical encoding of $c_j$ in training, only a label for $c$ that corresponds to the subdataset (that is, the label $c$ has no physical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:33:21.000442145Z",
     "start_time": "2024-01-24T13:33:20.929841721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the model class definition in an external file so they can be referenced outside this script\n",
    "phi_net = mlmodel.Phi_Net(options)\n",
    "h_net = mlmodel.H_Net_CrossEntropy(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:33:21.143730497Z",
     "start_time": "2024-01-24T13:33:20.956439996Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "criterion_h = nn.CrossEntropyLoss()\n",
    "optimizer_h = optim.Adam(h_net.parameters(), lr=options['learning_rate'])\n",
    "optimizer_phi = optim.Adam(phi_net.parameters(), lr=options['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Training Algorithm\n",
    "\n",
    "**Step 0: sample $c$, and sample $B+K$ data points in correponding subdataset $\\{x_i,c,f(x_i,c)\\}_{i}$**\n",
    "\n",
    "**Step 1: estimate $a$ using least-square**\n",
    "\n",
    "$K$ data points (sampled from the same wind condition $c$) are used to compute $a$ using least-squares, i.e., adaptation:\n",
    "$$\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        \\phi(x_1) \\\\ \\phi(x_2) \\\\ \\vdots \\\\ \\phi(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{\\Phi\\in\\mathbb{R}^{K\\times \\dim(a)}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        a_1 & \\cdots & a_{\\dim(y)} \n",
    "    \\end{bmatrix}}\n",
    "    _{a\\in\\mathbb{R}^{\\dim(a)\\times \\dim(y)}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "        f_1(x_1) & \\cdots & f_{\\dim(y)}(x_1) \\\\ f_1(x_2) & \\cdots & f_{\\dim(y)}(x_2) \\\\ \\vdots & \\vdots & \\vdots\\\\ f_1(x_K) & \\cdots & f_{\\dim(y)}(x_K) \n",
    "    \\end{bmatrix}}\n",
    "    _{Y\\in\\mathbb{R}^{K\\times \\dim(y)}}\n",
    "$$\n",
    "\n",
    "The least square solution is given by\n",
    "$$a=(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top Y$$\n",
    "Normalization on $a$ is implemented to avoid ambiguity of $\\phi(x)a$ (since $\\phi(x)a=(0.1\\phi(x))\\cdot(10a)$):\n",
    "$$a\\leftarrow \\gamma\\cdot\\frac{a}{\\|a\\|_F},\\quad\\text{if}\\,\\,\\|a\\|_F>\\gamma$$\n",
    "Note that $a$ is an implicit function of $\\phi$.\n",
    "\n",
    "**Step 2: fix $h$ and train $\\phi$**\n",
    "\n",
    "With this $a$, another $B$ data points (with same $c$) are used for gradient descent with loss\n",
    "$$\\mathcal{L}(\\phi)=\\|f(x)-\\phi(x)a\\|_2^2-\\alpha\\cdot\\|h(\\phi(x))-c\\|_2^2$$\n",
    "\n",
    "**Step 3: fix $\\phi$ and train discriminator $h$**\n",
    "\n",
    "Finally, these $B$ data points are used again for gradient descent on $h$ with loss\n",
    "$$\\mathcal{L}(h)=\\|h(\\phi(x))-c\\|_2^2$$\n",
    "We may run this step less frequently than step 2, to improve stability in training (a trick from GAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:35:29.016661754Z",
     "start_time": "2024-01-24T13:33:21.152055978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_freq = 50 # How often to save the model\n",
    "\n",
    "# Create some arrays to save training statistics\n",
    "Loss_f = [] # combined force prediction loss\n",
    "Loss_c = [] # combined adversarial loss\n",
    "\n",
    "# Loss for each subdataset \n",
    "Loss_test_nominal = [] # loss without any learning\n",
    "Loss_test_mean = [] # loss with mean predictor\n",
    "Loss_test_phi = [] # loss with NN\n",
    "for i in range(len(TestData)):\n",
    "    Loss_test_nominal.append([])\n",
    "    Loss_test_mean.append([])\n",
    "    Loss_test_phi.append([])\n",
    "\n",
    "# Training!\n",
    "for epoch in range(options['num_epochs']):\n",
    "    # Randomize the order in which we train over the subdatasets\n",
    "    arr = np.arange(options['num_c'])\n",
    "    np.random.shuffle(arr)\n",
    "\n",
    "    # Running loss over all subdatasets\n",
    "    running_loss_f = 0.0\n",
    "    running_loss_c = 0.0\n",
    "\n",
    "    for i in arr:\n",
    "        with torch.no_grad():\n",
    "            adaptloader = Adaptloader[i]\n",
    "            kshot_data = next(iter(adaptloader))\n",
    "            trainloader = Trainloader[i]\n",
    "            data = next(iter(trainloader))\n",
    "        \n",
    "        optimizer_phi.zero_grad()\n",
    "        \n",
    "        '''\n",
    "        Least-square to get $a$ from K-shot data\n",
    "        '''\n",
    "        X = kshot_data['input'] # K x dim_x\n",
    "        Y = kshot_data['output'] # K x dim_y\n",
    "        Phi = phi_net(X) # K x dim_a\n",
    "        Phi_T = Phi.transpose(0, 1) # dim_a x K\n",
    "        A = torch.inverse(torch.mm(Phi_T, Phi)) # dim_a x dim_a\n",
    "        a = torch.mm(torch.mm(A, Phi_T), Y) # dim_a x dim_y\n",
    "        if torch.norm(a, 'fro') > options['gamma']:\n",
    "            a = a / torch.norm(a, 'fro') * options['gamma']\n",
    "            \n",
    "        '''\n",
    "        Batch training \\phi_net\n",
    "        '''\n",
    "        inputs = data['input'] # B x dim_x\n",
    "        labels = data['output'] # B x dim_y\n",
    "        \n",
    "        c_labels = data['c'].type(torch.long)\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.mm(phi_net(inputs), a)\n",
    "        loss_f = criterion(outputs, labels)\n",
    "        temp = phi_net(inputs)\n",
    "        \n",
    "        loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "        loss_phi = loss_f - options['alpha'] * loss_c\n",
    "        loss_phi.backward()\n",
    "        optimizer_phi.step()\n",
    "        \n",
    "        '''\n",
    "        Discriminator training\n",
    "        '''\n",
    "        if np.random.rand() <= 1.0 / options['frequency_h']:\n",
    "            optimizer_h.zero_grad()\n",
    "            temp = phi_net(inputs)\n",
    "            \n",
    "            loss_c = criterion_h(h_net(temp), c_labels)\n",
    "            \n",
    "            loss_h = loss_c\n",
    "            loss_h.backward()\n",
    "            optimizer_h.step()\n",
    "        \n",
    "        '''\n",
    "        Spectral normalization\n",
    "        '''\n",
    "        if options['SN'] > 0:\n",
    "            for param in phi_net.parameters():\n",
    "                M = param.detach().numpy()\n",
    "                if M.ndim > 1:\n",
    "                    s = np.linalg.norm(M, 2)\n",
    "                    if s > options['SN']:\n",
    "                        param.data = param / s * options['SN']\n",
    "         \n",
    "        running_loss_f += loss_f.item()\n",
    "        running_loss_c += loss_c.item()\n",
    "    \n",
    "    # Save statistics\n",
    "    Loss_f.append(running_loss_f / options['num_c'])\n",
    "    Loss_c.append(running_loss_c / options['num_c'])\n",
    "    if epoch % 10 == 0:\n",
    "        print('[%d] loss_f: %.2f loss_c: %.2f' % (epoch + 1, running_loss_f / options['num_c'], running_loss_c / options['num_c']))\n",
    "\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for j in range(len(TestData)):\n",
    "            loss_nominal, loss_mean, loss_phi = mlmodel.error_statistics(TestData[j].X, TestData[j].Y, phi_net, h_net, options=options)\n",
    "            Loss_test_nominal[j].append(loss_nominal)\n",
    "            Loss_test_mean[j].append(loss_mean)\n",
    "            Loss_test_phi[j].append(loss_phi)\n",
    "\n",
    "    if epoch % model_save_freq == 0:\n",
    "        mlmodel.save_model(phi_net=phi_net, h_net=h_net, modelname=modelname + '-epoch-' + str(epoch), options=options)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5932d0aacfea12ae558253bdaac47ff94f362ee7b8ad1d98be4f4f6b94d2042e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "efbe968048790da5e26fe224257c59db0c3c3cb10bbad9ca12250f2d56e94a61"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
