# RL Planner for Quadrotor in Windy Environment

## Description
The `rl_policy` folder contains the bio-inspired trajectory planner training-related code. 

## Overview

The reinforcement learning (RL) planner for the quadrotor in a windy environment follows this general architecture:

1. **State**: The current state is fed into the RL planner.
2. **PPO (Proximal Policy Optimization)**: The RL planner uses PPO to generate normalized acceleration based on the state.
3. **Trajectory Generator**: The normalized acceleration is passed into the trajectory generator to generate the desired trajectory.
4. **MPC (Model Predictive Controller)**: The MPC controller uses the trajectory to control the quadrotor's movement.
5. **Environment (ENV)**: The quadrotor interacts with the environment based on the control commands generated by the MPC.

The main scripts include:

1. **mujocoEnv.py**: This script provides the gym-like interface of the MuJoCo environment to train an RL agent, including the `step` function, `reset` function, `get_reward` function, etc.
2. **train.py**: This script launches the training process using package `stable_baseline3`.
3. **experiment.py**: This script is used to test the trained model and collect the data.
4. **auto_test_example_fig_3**: This script demonstrates how trajectory planning and evaluation are carried out, corresponding to Fig. 3h.

After launching the Mujoco environment, we run script `train.py` to train model or `expriment.py` to test model.

### State Representation

The state of the quadrotor is represented by:

- **Relative Position** \( p \)
- **Velocity** \( v \)
- **Wind-effect acceleration** \( w \)

#### Normalized State

The state is normalized for better learning efficiency using the following equations:

\[
\hat{p} = \frac{p}{2 \cdot p_{\text{max}}}, \quad \hat{v} = \frac{v}{v_{\text{max}}}, \quad \hat{w} = \frac{w}{w_{\text{max}}}
\]

Where:
- \( p_{\text{max}} \) is the maximum possible position.
- \( v_{\text{max}} \) is the maximum velocity.
- \( w_{\text{max}} \) is the maximum wind force.

### Action Representation

The action given by the RL agent is the normalized reference acceleration \( \hat{a} \).

---

## Reward Function

The reward function consists of two stages, focusing on minimizing acceleration, position error, and velocity error.

### First Stage

The reward for the first stage is defined as:

\[
J_1 = \sum_{k = 1}^{T} \left( - \lambda_a \|a(k)\| - \lambda_p \|p(k) - p_{\text{target}}\| \right) -  \lambda_{tp} \|p(T) - p_{\text{target}}\| - \lambda_{tv} \|v(T)\| + r_{success}
\]

Where:
- \( \lambda_a, \lambda_p, \lambda_{tp}, \lambda_{tv} \) are positive constants.
- \( p_{\text{target}} \) is the target position.
- \( T \) is the time horizon.
- $r_{success}$ is the reward term for a successful landing

### Second Stage

The reward for the second stage incorporates additional factors such as decay and thrust error:

\[
J_2 = \sum_{k = 1}^{T} \left( -\lambda_a \|a(k)\| - \lambda_{\text{decay}} \cdot \lambda_p \|p(k) - p_{\text{target}}\| - \lambda_T \|a(k) - g - \omega(k)\| \right) -  \lambda_{tp} \|p(T) - p_{\text{target}}\| - \lambda_{tv} \|v(T)\| + r_{success}
\]

Where:
- \( \lambda_T \) is a positive constant.
- \( \lambda_{\text{decay}} \) is a positive constant that decays as the episode number increases.
- \( g \) represents the gravity, and \( \omega(k) \) is the wind force.

---

## Conclusion

The RL planner for a quadrotor in a windy environment leverages the PPO algorithm to optimize a complex reward function, taking into account multiple factors such as acceleration, position error, velocity error, and thrust adjustment. The goal is to generate a smooth and accurate trajectory for the quadrotor to land in the presence of wind disturbances.
